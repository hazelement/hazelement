{"pages":[{"title":"Contact","text":"Feel free to leave me a comment or contact me through my github profile.","tags":"pages","url":"https://hazelement.github.io/pages/contact.html"},{"title":"Distributed Task with Celery","text":"This article discusses code base setup and typical worker Nodes configurations for a distributed Celery worker system. Introduction Celery is a asynchronous task queue/job queue system that facilitates communications between Producer system and Consumer system. A typical setup can be shown as below. An advantage of this setup is that it enables separation of heavy loading operation from rest of application layers. It's easier to maintain and perform updates without affecting the entire system, essentially decouples system components. Challenges Task definition Task definition must be seen by both producer and consumer. But we would like to separate task implementation from producer to consumer. This is achieved through a shared celery task configuration code base between producer and consumer. Task queue configuration Different workers needs to be run on different host or queue. Otherwise worker might receive wrong task which were intended for other workers. Solution here is to use different queues for different tasks. Code base solutions Git setup Let's assume there are 2 git repos, one for Producer and one for Consumer . In order to shared Task definition, along with celery configuration, between Producer and Consumer , we create a 3rd git repo and add it as submodule for Producer and Consumer . So the setup looks like this. # producer git repo producer/ src/ ... celery_tasks/ # shared repo as submodule celery_config.py tasks.py utils.py ... ... # consumer git repo consumer/ src/ ... celery_tasks/ # shared repo as submodule celery_config.py tasks.py utils.py ... ... # shared celery task git repo celery_tasks/ celery_config.py tasks.py utils.py ... celery_config.py contains celery worker configurations which we will use to create celery app object. For example # celery_config.py import os from celery_tasks.rabbitmq_config import RABBITMQ_PORT , RABBITMQ_PWD , RABBITMQ_USER , RABBITMQ_HOST ## Broker settings. broker_url = f 'pyamqp://{RABBITMQ_USER}:{RABBITMQ_PWD}@{RABBITMQ_HOST}:{RABBITMQ_PORT}' result_backend = 'rpc://' Shared Task definition celery_tasks repo contains dummy task class definitions. These classes needs to define class name, unique name of the task, run method and its arguments. These classes are defined in tasks.py . For example, # tasks.py import celery class MagicCeleryTask ( celery . Task ): name = 'unique_magic_task' def run ( self , arg1 , arg2 , arg3 ): \"\"\" place holder method :param arg1: arg1 is for this :type arg1: int :param arg2: arg2 is for that :type arg2: int :param arg3: arg3 is for another :type arg3: int :return: result :rtype: str \"\"\" pass Consumers will inherit these classes and provide actual implementations, the run method, which enables it to perform real computations. Producers will utilize these dummy method to send tasks to celery backend. For example, in consumer code base, inherit MagicCeleryTask and implement actual computation. from celery_tasks.tasks import MagicCeleryTask class ActualMagicCeleryTask ( MagicCeleryTask ): def run ( self , arg1 , arg2 , arg3 ): \"\"\" actual implementation \"\"\" return arg1 + arg2 + arg3 In producer, we can import the class directly. from celery_tasks.tasks import MagicCeleryTask Celery app queue configuration Queue is configured based on task name to avoid worker conflicts. In celery_tasks , we created a create_worker_from method defined in utils.py . # utils.py def create_worker_from(WorkerClass, celery_config='celery_tasks.celery_config'): \"\"\" Create worker instance given WorkerClass :param WorkerClass: WorkerClass to perform task :type WorkerClass: subclass of celery.Task :param celery_config: celery config module, default 'celery_tasks.celery_config'. This depends on project path :type celery_config: str :return: celery app instance and worker task instance :rtype: tuple of (app, worker_task) \"\"\" assert issubclass(WorkerClass, celery.Task) app = celery.Celery() app.config_from_object(celery_config) app.conf.update(task_default_queue=WorkerClass.name) # update worker queue worker_task = app.register_task(WorkerClass()) return app, worker_task In producer and consumer, we use this method and task class to create celery app and worker class instances for each task. Worker class instances are used to produce and consume tasks. For example, in consumer, from celery_tasks.tasks import MagicCeleryTask from celery_tasks.utils import create_worker_from class ActualMagicCeleryTask ( MagicCeleryTask ): def run ( self , arg1 , arg2 , arg3 ): \"\"\" actual implementation \"\"\" return arg1 + arg2 + arg3 # create celery app app , _ = create_worker_from ( MagicCeleryTask ) # start worker args = [ '--without-gossip' , '--without-mingle' , '--without-heartbeat' ] app . worker_main ( argv = args ) For example, in producer, from celery_tasks.tasks import MagicCeleryTask from celery_tasks.utils import create_worker_from # create worker _ , worker = create_worker_from ( MagicCeleryTask ) # send task to queue and get result result = worker . delay ( 1 , 2 , 3 ) . get ()","tags":"celery","url":"https://hazelement.github.io/distributed-task-with-celery.html"},{"title":"Use Pihole to Prevent Windows 10 Auto Update","text":"Pihole For those who haven't heard about Pihole, it's a light DNS service that can run on a device as small as a raspberry pi. It can be used as a DNS service to block certain unwanted domains. Most popular usage is to block advertisement and online tracking services. The GitHub page is here . Running a Pihole in docker We will be using docker to run a Pihole service. Pihole has an official docker image that can be found here . A docker-compose file is also provided on their GitHub page. It's posted here for convenience. version: \"3\" # More info at https://github.com/pi-hole/docker-pi-hole/ and https://docs.pi-hole.net/ services: pihole: container_name: pihole image: pihole/pihole:latest ports: - \"53:53/tcp\" - \"53:53/udp\" - \"67:67/udp\" - \"80:80/tcp\" - \"443:443/tcp\" environment: TZ: 'America/Chicago' # WEBPASSWORD: 'set a secure password here or it will be random' # Volumes store your data between container upgrades volumes: - './etc-pihole/:/etc/pihole/' - './etc-dnsmasq.d/:/etc/dnsmasq.d/' dns: - 127.0.0.1 - 1.1.1.1 # Recommended but not required (DHCP needs NET_ADMIN) # https://github.com/pi-hole/docker-pi-hole#note-on-capabilities cap_add: - NET_ADMIN restart: unless-stopped Let's assume Pihole is running on a local server with IP address 192.168.0.254 . On web browser, go to that IP address. A pihole page should show up. Click on Did you mean to go to the admin panel? . This will bring us to the admin page. Click on login in and enter the password defined in the docker-compose.yml file. Configure Pihole to block Windows 10 update server Pihole's blocking mechanism are based on blacklist and whitelist. Navigate to Setting > Blocklists . Here you can add as many lists as you want. There are many community list out there that block different types of services. You can see I have a lot of them added. At the bottom of the list, we can see a place where we can add more lists. The URL we are using is from here . Many thanks to krathalan and in2016minewastaken from reddit. Enter the url and click on Save and Update to update the rules, Configure Windows to use the Pihole as DNS server There are 2 options here. Configure specific machines we would like to prevent auto update or apply this on all Windows 10 machines on the local network. To configure specific machines, just configure the DNS server on each machine to 192.168.0.254 . To configure for all machines on the local network, configure router's DNS server to 192.168.0.254 . Refer to your router manual to setup this. And that's it! Next time Windows machines try to get an update, it will hit a wall at our DNS server and fail.","tags":"pihole","url":"https://hazelement.github.io/use-pihole-to-prevent-windows-10-auto-update.html"},{"title":"Kubernetes Persistent Volume","text":"This article is a summary of the tutorial . It utilizes manifest file to create deployments and services. Objectives In this tutorial, we will deploy a WordPress and MySQL service using persistent volumes for data storage. A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass . A PersistentVolumeClaim (PVC) is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods. Create PersistentVolumeClaims and PersistentVolumes MySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster's default StorageClass is used instead. When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration. Create a Secret for MySQL Password A Secret is an object that stores a piece of senstive information like a password or a key. Once a Secret is created, it can be refer to in manifest files like an environment variable. Create a Secret object using the following command, replacing YOUR_PASSWORD with your own password. kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD This creates a mysql-pass object with key value pair password and YOUR_PASSWORD . Verify the Secret exists using the following command, $ kubectl get secrets NAME TYPE DATA AGE mysql-pass Opaque 1 42s Notice that the content is not shown. Deploy MySQl The following manifest, mysql-deployment.yaml , describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The MYSQL_ROOT_PASSWORD environment variable sets the database password from the Secret. apiVersion : v1 kind : Service metadata : name : wordpress - mysql labels : app : wordpress spec : ports : - port : 3306 selector : app : wordpress tier : mysql clusterIP : None --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mysql - pv - claim labels : app : wordpress spec : accessModes : - ReadWriteOnce resources : requests : storage : 20 Gi --- apiVersion : apps /v1 # for versions before 1.9.0 use apps/ v1beta2 kind : Deployment metadata : name : wordpress - mysql labels : app : wordpress spec : selector : matchLabels : app : wordpress tier : mysql strategy : type : Recreate template : metadata : labels : app : wordpress tier : mysql spec : containers : - image : mysql : 5.6 name : mysql env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mysql - pass # Secret object key : password # key value pair value ports : - containerPort : 3306 name : mysql volumeMounts : - name : mysql - persistent - storage mountPath : /var/lib/ mysql volumes : - name : mysql - persistent - storage persistentVolumeClaim : claimName : mysql - pv - claim Deploy MySQL using the yaml file, $ kubectl create -f mysql-deployment.yaml Verify that a PersistentVolume got dynamically provisioned, $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-9ac5609d-4f32-11e9-9123-025000000001 20Gi RWO hostpath 8s Verify pods is running, $ kubectl get pods NAME READY STATUS RESTARTS AGE wordpress-mysql-bcc89f687-b5vb2 1 /1 Running 0 1m Deploy WordPress The following manifest file, wordpress-deployment.yaml , describes a single-instance WordPress Deployment and Service. It uses PVC for persistent storage and a Secret for password. It also use type: LoadBalancer . This setting exposes WordPress to traffic from outside of the cluter. apiVersion : v1 kind : Service metadata : name : wordpress labels : app : wordpress spec : ports : - port : 80 selector : app : wordpress tier : frontend type : LoadBalancer --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : wp - pv - claim labels : app : wordpress spec : accessModes : - ReadWriteOnce resources : requests : storage : 20 Gi --- apiVersion : apps /v1 # for versions before 1.9.0 use apps/ v1beta2 kind : Deployment metadata : name : wordpress labels : app : wordpress spec : selector : matchLabels : app : wordpress tier : frontend strategy : type : Recreate template : metadata : labels : app : wordpress tier : frontend spec : containers : - image : wordpress : 4.8 - apache name : wordpress env : - name : WORDPRESS_DB_HOST value : wordpress - mysql - name : WORDPRESS_DB_PASSWORD valueFrom : secretKeyRef : name : mysql - pass key : password ports : - containerPort : 80 name : wordpress volumeMounts : - name : wordpress - persistent - storage mountPath : /var/www/ html volumes : - name : wordpress - persistent - storage persistentVolumeClaim : claimName : wp - pv - claim Create a WordPress Service and Deployment form the file wordpress-deployment.yaml file, kubectl create -f wordpress-deployment.yaml Verify that a PersistentVolume got dynamically provisioned, $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-9ac5609d-4f32-11e9-9123-025000000001 20Gi RWO hostpath 8m wp-pv-claim Bound pvc-cf4a8bdc-4f33-11e9-9123-025000000001 20Gi RWO hostpath 12s Verify that Service is running, $ kubectl get services wordpress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE wordpress LoadBalancer 10 .110.116.160 localhost 80 :31111/TCP 50s Check localhost on the browser and a WordPress page should show up. Cleaning up Run the following command to delete Secret, kubectl delete secret mysql-pass Run following command to delete all Deployments and Services, kubectl delete deployment -l app=wordpress; kubectl delete service -l app=wordpress Run the following commands to delete the PersistentVolumeClaims. kubectl delete pvc -l app=wordpress","tags":"Kubernetes","url":"https://hazelement.github.io/kubernetes-persistent-volume.html"},{"title":"Kubernetes Stateful Application","text":"This article is a summary of the tutorial . It utilizes manifest file to create deployments and services. Most of the time, we need to have persistent data saved at a location and when the service is restarted, the data was saved and can be loaded to restore its latest state. This is achived through stateful application in Kubernetes. Objective This tutorial covers basic steps deploy a simple web application using StatefulSet . The website is served in a html file by nginx. Create a StatefulSet Manage Pods through StatefulSet Delete a StatefulSet Scale a StatefulSet Update a StatefulSet's Pods Creating a StatefulSet Using the yaml file below, we create a headless service, nginx , to publish the IP addresses of Pods in the StatefulSet, web . The manifest file web.yaml . apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps / v1 kind : StatefulSet metadata : name : web spec : serviceName : \"nginx\" replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s . gcr . io / nginx - slim : 0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/ html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1 Gi In this tutorial, we will use two terminal windows. One uses kubectl get pods -w -l app=nginx command to get live update from Kubernetes service about the status of our pods. The other terminal window is used to execute command the deploy, update and delete Pods and StatefulSets. In the first terminal window execiute this command to watch the creation of the StatefulSet's Pods. kubectl get pods -w -l app=nginx In the second terminal, use kubectl create to create the headless Service and StatefulSet defined in web.yaml . $ kubectl create -f web.yaml service/nginx created statefulset.apps/web created Check the running service, $ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx ClusterIP None <none> 80 /TCP 12s Check running statefulset $ kubectl get statefulset web NAME DESIRED CURRENT AGE web 2 1 20s On the first terminal, we should see the Pods being deployed in action. $ kubectl get pods -w -l app = nginx NAME READY STATUS RESTARTS AGE web-0 0 /1 Pending 0 0s web-0 0 /1 Pending 0 0s web-0 0 /1 ContainerCreating 0 0s web-0 1 /1 Running 0 19s web-1 0 /1 Pending 0 0s web-1 0 /1 Pending 0 0s web-1 0 /1 ContainerCreating 0 0s web-1 1 /1 Running 0 18s Pods in a StatefulSet Pods in a StatefulSet have a unique ordinal index and a stable network identity. Examping the Pod's Ordinal Index Get the StatefulSet's Pods. $ kubectl get pods -l app = nginx NAME READY STATUS RESTARTS AGE web-0 1 /1 Running 0 1m web-1 1 /1 Running 0 1m The Pods' names take the form <statefulset name>-<ordinal index> . Since the web StatefulSet has two replicas, it creates two Pods, web-0 and web-1 . Each Pod has a stable hostname based on its ordinal index. for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done web-0 web-1 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.6 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.6 Manage Pods in StatefulSet In second terminal, use kubectl delete to delete all Pods in the StatefulSet. $ kubectl delete pod -l app = nginx pod \"web-0\" deleted pod \"web-1\" deleted This will delete both pods but StatefulSet will restart then. The process is shown in the first terminal. $ kubectl get pod -w -l app = nginx NAME READY STATUS RESTARTS AGE web-0 0 /1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1 /1 Running 0 2s web-1 0 /1 Pending 0 0s web-1 0 /1 Pending 0 0s web-1 0 /1 ContainerCreating 0 0s web-1 1 /1 Running 0 34s Try print out the hostname again, for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done web-0 web-1 kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.7 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 The hostname persists, but IP addresses associated with the Pods maybe change. This is why it is important not to configure other applications to connect to Pods in a StatefulSet by IP address. Writing to stable storage We will be writing a text to the index page in the persistent volume. Get the PersistentVolumeClaims for web-0 and web-1 . $ kubectl get pvc -l app = nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 48s www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi The NGINX webservers, by default, will serve an index file at /usr/share/nginx/html/index.html . The volumeMounts field in the StatefulSets spec ensures that the /usr/share/nginx/html directory is backed by a PersistentVolume. Write the Pods' hostnames to their index.html files and verify that the NGINX webservers serve the hostnames. for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 Now let's delete the Pods: $ kubectl delete pod -l app = nginx pod \"web-0\" deleted pod \"web-1\" deleted StatefulSet will recreate these Pods and mount the same persistent volume onto them. Thus the changes we've written to index.html should still be there. Check the index.html again, for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 Scaling a StatefulSet Scaling up In terminal 2, user kubectl scale to scale the number of replicas to 5. $ kubectl scale sts web --replicas = 5 statefulset.apps/web scaled In terminal 1, this change should show up kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2h web-1 1/1 Running 0 2h NAME READY STATUS RESTARTS AGE web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 ContainerCreating 0 0s web-3 1/1 Running 0 18s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 0s web-4 1/1 Running 0 19s Scaling down Use kubectl patch to scale the StatefulSet back down to three replicas. $ kubectl patch sts web -p '{\"spec\":{\"replicas\":3}}' statefulset.apps/web patched In temrinal 1, the process should show up. kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3h web-1 1/1 Running 0 3h web-2 1/1 Running 0 55s web-3 1/1 Running 0 36s web-4 0/1 ContainerCreating 0 18s NAME READY STATUS RESTARTS AGE web-4 1/1 Running 0 19s web-4 1/1 Terminating 0 24s web-4 1/1 Terminating 0 24s web-3 1/1 Terminating 0 42s web-3 1/1 Terminating 0 42s But the PersistenVolumes mounted to the Pods are not deleted when the StatefulSet's Pods are deleted. This is true when Pod deletion is caused by scaling the StatefulSet down. $ kubectl get pvc -l app = nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-2 Bound pvc-e1125b27-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-3 Bound pvc-e1176df6-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-4 Bound pvc-e11bb5f8-b508-11e6-932f-42010a800002 1Gi Updating StatefulSets The update strategy is determined by the spec.updateStrategy field of the StatefulSet API object. This feature can be used to upgrade the container images, resource requests and/or limits, labels, and annotations of the Pods in a StatefulSet. There are two valid update strategies, RollingUpdate and OnDelete . RollingUpdate strategy is the default for StatefulSets. Rollin gupdate RollingUpdate strategy update all Pods in a StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees. Patch the web StatefulSet to apply the strategy, $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\"}}}' statefulset.apps/web patched In terminal 2, patch the web to change the container image. $ kubectl patch statefulset web --type = 'json' -p = '[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"gcr.io/google_containers/nginx-slim:0.8\"}]' statefulset.apps/web patched In terminal 1, the process should be shown, kubectl get po -l app=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 7m web-1 1/1 Running 0 7m web-2 1/1 Running 0 8m web-2 1/1 Terminating 0 8m web-2 1/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-1 1/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 6s web-0 1/1 Terminating 0 7m web-0 1/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 10s From the log, we can confirm that the Pods are updated in reverse ordinal order. The StatefulSet controller terminates each Pod, and waits for it to transition to Running and Ready prior to updating the next Pod. Note that, even though the StatefulSet controller will not proceed to update the next Pod until its ordinal successor is Running and Ready, it will restore any Pod that fails during the update to its current version. Pods that have already received the update will be restored to the updated version, and Pods that have not yet received the update will be restored to the previous version. In this way, the controller attempts to continue to keep the application healthy and the update consistent in the presence of intermittent failures. Check the Pods' images: for p in 0 1 2; do kubectl get po web-$p --template ' {{ range $ i , $ c := .spec.containers }}{{ $ c.image }}{{ end }} '; echo; done k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 kubectl rollout status sts/<name> can also view the status of a rolling update. Staging an update Staging an update can be achived by using the partition parameter of the RollingUpdate update strategy. It will keep all of the Pods in the StatefulSet at the current version while allowing mutation to the StatefulSet's .spec.template . Use the following command to issue the patch. kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":3}}}}' statefulset.apps/web patched \"partition\":3 covers the first 3 Pods in ordinal order. In this case web-2 is covered. Change the container's image, kubectl patch statefulset web --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"k8s.gcr.io/nginx-slim:0.7\"}]' statefulset.apps/web patched Delete the Pod in the StatefulSet, kubectl delete po web-2 pod \"web-2\" deleted Wait for StatefulSet to restart the Pod. Check the Pod's image again. kubectl get po web-2 --template ' {{ range $ i , $ c := .spec.containers }}{{ $ c.image }}{{ end }} ' k8s.gcr.io/nginx-slim:0.8 Notice that it's still on version 0.8 . StatefulSet controller restored the Pod with its original container. This is because the ordinal of the Pod is less than the partition specified by the updateStrategy . Rolling out a Canary To roll out a canary to test, we can dcrementing the partition number. $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":2}}}}' statefulset.apps/web patched This will cover web-0 and web-1 , web-2 is no longer covered and will be restarted. Wait for web-2 to be Running and Ready. $ kubectl get po -l app = nginx -w NAME READY STATUS RESTARTS AGE web-0 1 /1 Running 0 4m web-1 1 /1 Running 0 4m web-2 0 /1 ContainerCreating 0 11s web-2 1 /1 Running 0 18s Check the Pod's container, kubectl get po web-2 --template ' {{ range $ i , $ c := .spec.containers }}{{ $ c.image }}{{ end }} ' k8s.gcr.io/nginx-slim:0.7 When you changed the partition , the StatefulSet controller automatically updated the web-2 Pod because the Pod's ordinal was greater than or equal to the partition . Verify web-1 is still on version 0.8 by deleting it and wait for StatefulSet to restart it. kubectl delete po web-1 pod \"web-1\" deleted Wait for web-1 Pod to be Running and Ready, kubectl get po -l app=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 0/1 Terminating 0 6m web-2 1/1 Running 0 2m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s Check the image version, kubectl get po web-1 --template ' {{ range $ i , $ c := .spec.containers }}{{ $ c.image }}{{ end }} ' k8s.gcr.io/nginx-slim:0.8 web-1 was restored to original configuration because the Pod's ordinal was less than the parition. All Pods with an ordinal that is greater than or equial to the partiion will be updated when the Statefulset's .spec.template is updated. If a Pod that has an ordinal less than the partition is deleted or otherwise terminated, it will be restored to its original configuration. Phased roll outs Now that we've tesd the canary update on web-2, it's time to roll it out to all other Pods. The perform the phased roll out, update the partition to 0 . This means none of the Pods are covered under partition . $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":0}}}}' statefulset.apps/web patched Wait for all Pods to be come Running and Ready. kubectl get po -l app=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3m web-1 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 2m web-1 1/1 Running 0 18s web-0 1/1 Terminating 0 3m web-0 1/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 3s Check Pod's container image versions. for p in 0 1 2; do kubectl get po web-$p --template ' {{ range $ i , $ c := .spec.containers }}{{ $ c.image }}{{ end }} '; echo; done k8s.gcr.io/nginx-slim:0.7 k8s.gcr.io/nginx-slim:0.7 k8s.gcr.io/nginx-slim:0.7 Now all Pods are updated. Deleting StatefulSets StatefulSet supports both Non-Cascading and Cascading deletion. In a Non-Cascading Delete, the StatefulSet's Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are deleted. Non-Cascading Delete Use kubectl delete to delete the StatefulSet. Make sure to supply the --cascade=false paramter to the command. $ kubectl delete statefulset web --cascade = false statefulset.apps \"web\" deleted Get the Pods, $ kubectl get pods -l app = nginx NAME READY STATUS RESTARTS AGE web-0 1 /1 Running 0 6m web-1 1 /1 Running 0 7m web-2 1 /1 Running 0 5m All Pods are still running but they shouldn't be recreated if deleted as StatefulSet is deleted. Delete the first Pod, $ kubectl delete pod web-0 pod \"web-0\" deleted List the Pods, kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 10m web-2 1/1 Running 0 7m web-0 is removed for good. Now let's restart the StatefulSet, $ kubectl create -f web.yaml statefulset.apps/web created Error from server ( AlreadyExists ) : error when creating \"web.yaml\" : services \"nginx\" already exists Examp the output of the kubectl get , $ kubectl get pods -w -l app = nginx NAME READY STATUS RESTARTS AGE web-1 1 /1 Running 0 16m web-2 1 /1 Running 0 2m NAME READY STATUS RESTARTS AGE web-0 0 /1 Pending 0 0s web-0 0 /1 Pending 0 0s web-0 0 /1 ContainerCreating 0 0s web-0 1 /1 Running 0 18s web-2 1 /1 Terminating 0 3m web-2 0 /1 Terminating 0 3m web-2 0 /1 Terminating 0 3m web-2 0 /1 Terminating 0 3m The manifest requested replica of 2. web-0 was removed before, thus recreated. web-1 exists and is adopted. web-2 is beyond the replica 2 and is terminated. for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 Notice the hostname in index.html still persists. Cascade delete Use the same delete command but without --cascade=false . $ kubectl delete statefulset web statefulset.apps \"web\" deleted Examp the output of kubectl get pods -w -l app=nginx , kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 11m web-1 1/1 Running 0 27m NAME READY STATUS RESTARTS AGE web-0 1/1 Terminating 0 12m web-1 1/1 Terminating 0 29m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m The Pods are deleted one at a time, with respect to the reverse order of their ordinal indices. Note that, cascading delete will delete the StatefulSet and its Pods, it will not delete the headless Service associated with the StatefulSet. We need to delete the nginx Service manually. $ kubectl delete service nginx service \"nginx\" deleted Pod management policy For some distributed systems, the StatefulSet ordering guarantees are unnecessary and/or undesierable. This can be changed using .spec.podManagementPolicy in the StatefulSet API object. OrderedReady Pod Management OrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above. Parallel Pod Management Parallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod. For example, apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps / v1 kind : StatefulSet metadata : name : web spec : serviceName : \"nginx\" podManagementPolicy : \"Parallel\" replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s . gcr . io / nginx - slim : 0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/ html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1 Gi","tags":"Kubernetes","url":"https://hazelement.github.io/kubernetes-stateful-application.html"},{"title":"Kubernetes Stateless Application","text":"This article is a summary of the tutorial . It utilizes manifest file to create deployments and services. A stateless application doesn't save its data to hard drive, thus every time the application is restarted. It returns back to its original state. Objective This tutorial covers basic steps to setup a guest book in stateless manner. It's a guest book application with redis backend. Start up redis master. Start up redis slave. Start up guest book front end. Expose and view the Frontend service. Clean up. redis backend service Redis master Create a redis master service has 2 steps. Create redis master deployment Create redis master service based on the deployment in step 1. Create redis master deployment To create a redis master deployment, use the following manifest file, redis-master-deployment.yaml . apiVersion : apps /v1 # for versions before 1.9.0 use apps/ v1beta2 kind : Deployment metadata : name : redis - master labels : app : redis spec : selector : matchLabels : app : redis role : master tier : backend replicas : 1 template : metadata : labels : app : redis role : master tier : backend spec : containers : - name : master image : k8s . gcr . io / redis : e2e # or just image : redis resources : requests : cpu : 100 m memory : 100 Mi ports : - containerPort : 6379 In terminal, issue this command kubectl apply -f redis-master-deployment.yaml . Ensure the deployment is running. $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-55db5f7567-wmrjk 0 /1 ContainerCreating 0 8s To view the logs of the master pod: $ kubectl logs -f redis-master-55db5f7567-wmrjk _._ _.- `` __ '' -._ _.- `` ` . ` _. '' -._ Redis 2 .8.19 ( 00000000 /0 ) 64 bit .- `` .- ``` . ``` \\/ _.,_ '' -._ ( ' , .-` | `, ) Running in stand alone mode |`-._`-...-` __...-.``-._|' ` _.- '| Port: 6379 | `-._ `._ / _.-' | PID: 1 ` -._ ` -._ ` -./ _.- ' _.-' | ` -._ ` -._ ` -.__.- ' _.-' _.- '| | `-._`-._ _.-' _.- ' | http://redis.io `-._ `-._`-.__.-' _.- ' _.-' | ` -._ ` -._ ` -.__.- ' _.-' _.- '| | `-._`-._ _.-' _.- ' | `-._ `-._`-.__.-' _.- ' _.-' ` -._ ` -.__.- ' _.-' ` -._ _.- ' `-.__.-' [ 1 ] 13 Mar 20 :29:20.725 # Server started, Redis version 2.8.19 [ 1 ] 13 Mar 20 :29:20.726 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. [ 1 ] 13 Mar 20 :29:20.726 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. [ 1 ] 13 Mar 20 :29:20.726 * The server is now ready to accept connections on port 6379 Create redis master service For other applications to communicate with redis master, we need to apply a Service to proxy the traffic to the redis master pod. Use the following manifest file, redis-master-service.yaml , to define a Service . apiVersion : v1 kind : Service metadata : name : redis - master labels : app : redis role : master tier : backend spec : ports : - port : 6379 targetPort : 6379 selector : app : redis role : master tier : backend Start the service using the apply method. kubectl apply -f redis-master-service.yaml Make sure the service is running: $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 19d redis-master ClusterIP 10 .110.130.106 <none> 6379 /TCP 5s Create redis slave deployment To create a redis slave deployment, use the following manifest file, redis-slave-deployment.yaml . apiVersion : apps /v1 # for versions before 1.9.0 use apps/ v1beta2 kind : Deployment metadata : name : redis - slave labels : app : redis spec : selector : matchLabels : app : redis role : slave tier : backend replicas : 2 template : metadata : labels : app : redis role : slave tier : backend spec : containers : - name : slave image : gcr . io /google_samples/g b - redisslave : v1 resources : requests : cpu : 100 m memory : 100 Mi env : - name : GET_HOSTS_FROM value : dns # Using ` GET_HOSTS_FROM = dns ` requires your cluster to # provide a dns service . As of Kubernetes 1.3 , DNS is a built - in # service launched automatically . However , if the cluster you are using # does not have a built - in DNS service , you can instead # access an environment variable to find the master # service 's host. To do so, comment out the ' value : dns ' line above , and # uncomment the line below : # value : env ports : - containerPort : 6379 Start up the deployment, kubectl apply -f redis-slave-deployment.yaml Check running pods, $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-55db5f7567-wmrjk 1 /1 Running 0 23m redis-slave-584c66c5b5-ghrsz 0 /1 ContainerCreating 0 5s redis-slave-584c66c5b5-tpd4l 0 /1 ContainerCreating 0 5s Create redis slave service To open up slave deployment for other applications to communicate, we use a Service like the master deployment, redis-slave-service.yaml . apiVersion : v1 kind : Service metadata : name : redis - slave labels : app : redis role : slave tier : backend spec : ports : - port : 6379 selector : app : redis role : slave tier : backend Start the service, kubectl apply -f redis-slave-service.yaml . Check running services, $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 19d redis-master ClusterIP 10 .110.130.106 <none> 6379 /TCP 3m redis-slave ClusterIP 10 .108.183.147 <none> 6379 /TCP 7s PHP guestbook frontend Create frontend deployment To create a deployment, use the manifest file, frontend-deployment.yaml . apiVersion : apps /v1 # for versions before 1.9.0 use apps/ v1beta2 kind : Deployment metadata : name : frontend labels : app : guestbook spec : selector : matchLabels : app : guestbook tier : frontend replicas : 3 template : metadata : labels : app : guestbook tier : frontend spec : containers : - name : php - redis image : gcr . io /google-samples/g b - frontend : v4 resources : requests : cpu : 100 m memory : 100 Mi env : - name : GET_HOSTS_FROM value : dns # Using ` GET_HOSTS_FROM = dns ` requires your cluster to # provide a dns service . As of Kubernetes 1.3 , DNS is a built - in # service launched automatically . However , if the cluster you are using # does not have a built - in DNS service , you can instead # access an environment variable to find the master # service 's host. To do so, comment out the ' value : dns ' line above , and # uncomment the line below : # value : env ports : - containerPort : 80 Start the deployment, kubectl apply -f frontend-deployment.yaml Quest the list of pods to verify the frontend replicas are running: $ kubectl get pods -l app = guestbook -l tier = frontend NAME READY STATUS RESTARTS AGE frontend-5c548f4769-bsjjv 0 /1 ContainerCreating 0 9s frontend-5c548f4769-mjq4h 0 /1 ContainerCreating 0 9s frontend-5c548f4769-sljfb 0 /1 ContainerCreating 0 9s Create frontend service The redis-slave and redis-master services are only accessible within the container cluster because default type for a service is CluterIP. ClusterIP provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster. To allow guests to be able to access the guestbook, we need to configure the frontend service to the external internet. This is achived through type: NodePort or type: LoadBalancer . We use LoadBalancer here as an example. frontend-service.yaml . apiVersion : v1 kind : Service metadata : name : frontend labels : app : guestbook tier : frontend spec : # comment or delete the following line if you want to use a LoadBalancer # type : NodePort # if your cluster supports it , uncomment the following to automatically create # an external load - balanced IP for the frontend service . type : LoadBalancer ports : - port : 80 selector : app : guestbook tier : frontend Start a the service, kubectl apply -f frontend-service.yaml List running services to verify, $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE frontend LoadBalancer 10 .100.241.58 localhost 80 :31546/TCP 32s kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 19d redis-master ClusterIP 10 .110.130.106 <none> 6379 /TCP 6m redis-slave ClusterIP 10 .108.183.147 <none> 6379 /TCP 3m View the front end To view the frontend, go to localhost:80 on the browser and the frontend website should show up. Scale the web frontend Run the following command to scale up the number of frontend Pods, kubectl scale deployment frontend --replicas=5 Query the list of Pods to verify the number of frontend Pods running, $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-3823415956-70qj5 1 /1 Running 0 5s frontend-3823415956-dsvc5 1 /1 Running 0 54m frontend-3823415956-k22zn 1 /1 Running 0 54m frontend-3823415956-w9gbt 1 /1 Running 0 54m frontend-3823415956-x2pld 1 /1 Running 0 5s redis-master-1068406935-3lswp 1 /1 Running 0 56m redis-slave-2005841000-fpvqc 1 /1 Running 0 55m redis-slave-2005841000-phfv9 1 /1 Running 0 55m To scale down the number of frontend Pods: kubectl scale deployment frontend --replicas=2 To verify, $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-3823415956-k22zn 1 /1 Running 0 1h frontend-3823415956-w9gbt 1 /1 Running 0 1h redis-master-1068406935-3lswp 1 /1 Running 0 1h redis-slave-2005841000-fpvqc 1 /1 Running 0 1h redis-slave-2005841000-phfv9 1 /1 Running 0 1h Clean up Run the following commands to delete all Pods, Deployments and Services, $ kubectl delete deployment -l app = redis deployment.extensions \"redis-master\" deleted deployment.extensions \"redis-slave\" deleted $ kubectl delete service -l app = redis service \"redis-master\" deleted service \"redis-slave\" deleted $ kubectl delete deployment -l app = guestbook deployment.extensions \"frontend\" deleted $ kubectl delete service -l app = guestbook service \"frontend\" deleted Query the list of Pods to verify all pods are terminated, $ kubectl get pods No resources found.","tags":"Kubernetes","url":"https://hazelement.github.io/kubernetes-stateless-application.html"},{"title":"Kubernetes Workflow","text":"This article summarizes the basic workflow when working with a kubernetes cluster Basic deployment Run a node To run the node using a an image, run this command kubectl run hello-world --replicas=5 --labels=\"run=load-balancer-example\" --image=gcr.io/google-samples/node-hello:1.0 --port=8080 This will create a hello-world Deployment object with 5 replicas. Display information about the Deployment . kubectl get deployments hello-world kubectl describe deployments hello-world Use a service to expose the deployment Create a service to expose the Deployment : kubectl expose deployment hello-world --type=LoadBalancer --name=my-service This will create a LoadBalance service that manages the hello-world replicas and expose port 8080 to outside world. kubectl get services my-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.3.245.137 104.198.205.71 8080/TCP 54s Try access the service from outside using. curl http://<external-ip>:<port> Hello Kubernetes! Scale deployment To change the number of replicas in the deployment object. use this command: $ kubectl scale deployments/hello-world --replicas = 8 deployment.extensions \"hello-world\" scaled $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hello-world 8 8 8 8 15m $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-5b446dd74b-72jp4 1 /1 Running 0 16m hello-world-5b446dd74b-7xh77 1 /1 Running 0 9s hello-world-5b446dd74b-87hlb 1 /1 Running 0 16m hello-world-5b446dd74b-c4q4t 1 /1 Running 0 9s hello-world-5b446dd74b-jzp9d 1 /1 Running 0 16m hello-world-5b446dd74b-plvrp 1 /1 Running 0 9s hello-world-5b446dd74b-pxg2w 1 /1 Running 0 16m hello-world-5b446dd74b-vrm4r 1 /1 Running 0 16m hello-world is the pod we are running and it's a Deployment , hence deployments/hello-world Delete service and deployment To delete the Service, enter this command: kubectl delete services my-service To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command: kubectl delete deployment hello-world","tags":"Kubernetes","url":"https://hazelement.github.io/kubernetes-workflow.html"},{"title":"Kubernetes Basics","text":"This article covers some basic commands and instructions to deploy a kubernetes app. Some notes and images are taken from https://kubernetes.io/docs/tutorials . Basic concept Nodes A kubernetes cluster is consist of Nodes . A node can be a VM or a physical machine. To check current nodes issue this command: $ kubectl get nodes NAME STATUS ROLES AGE VERSION docker-for-desktop Ready master 30m v1.10.11 Each cluster should have one master node with a 0 or a few slave nodes. Pods A node is consists of one or more Pods . To get list of running pods, issue this command: $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5c69669756-rmxrn 1 /1 Running 0 45m A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Those resources include: Shared storage, as Volumes Networking, as a unique cluster IP address Information about how to run each container, such as the container image version or specific ports to use Services A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML (preferred) or JSON, like all Kubernetes objects. A Service routes traffic across a set of Pods. Services are the abstraction that allow pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) is handled by Kubernetes Services. Services match a set of Pods using labels and selectors, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways: Designate objects for development, test, and production Embed version tags Classify an object using tags Networking Pods running in side Kubernetes are running on a prviate, isolated network. By default, they are visible from other pods and services within the same cluster, but not outside. To quickly open on communication to outside world, kubectl can create a proxy to foward communications, kubectl proxy will create a proxy at http://localhost:8001/version . The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy. To access each individual pod, we need to get pod name. export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') . Then we can make a HTTP request to the application in that pod. curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/ Tutorial Make an deployment Run this commmand to make a new deployment. kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080 Get the name of running pod, export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') . Check application configuration Let's verify that the application we deployed in the previous scenario is running. We'll use the kubectl get command and look for existing Pods: $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5c69669756-rmxrn 1 /1 Running 0 1h Next, to view what containers are inside that Pod and what images are used to build those containers we run the describe pods command: kubectl describe pods . View container logs Anything that the application would normally send to STDOUT becomes logs for the container within the Pod. We can retrieve these logs using the kubectl logs command: kubectl logs $POD_NAME Note: We don't need to specify the container name, because we only have one container inside the pod. Executing command on the container We can execute commands directly on the container once the Pod is up and running. For this, we use the exec command and use the name of the Pod as a parameter. Let's list the environment variables: $ kubectl exec $POD_NAME env PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME = kubernetes-bootcamp-5c69669756-rmxrn KUBERNETES_PORT = tcp://10.96.0.1:443 KUBERNETES_PORT_443_TCP = tcp://10.96.0.1:443 KUBERNETES_PORT_443_TCP_PROTO = tcp KUBERNETES_PORT_443_TCP_PORT = 443 KUBERNETES_PORT_443_TCP_ADDR = 10 .96.0.1 KUBERNETES_SERVICE_HOST = 10 .96.0.1 KUBERNETES_SERVICE_PORT = 443 KUBERNETES_SERVICE_PORT_HTTPS = 443 NPM_CONFIG_LOGLEVEL = info NODE_VERSION = 6 .3.1 HOME = /root Again, worth mentioning that the name of the container itself can be omitted since we only have a single container in the Pod. Next let's start a bash session in the Pod's container: kubectl exec -ti $POD_NAME bash . We have now an open console on the container. To close the console, use exit . Create a new service To list current services in the cluster: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 1h Cluter services is created by default. To create a new service using running pods: kubectl expose deployment/kubernetes-bootcamp --type=\"NodePort\" --port 8080 List services again: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 1h kubernetes-bootcamp NodePort 10 .104.156.85 <none> 8080 :30527/TCP 2s Notice that the new service kubernetes-bootcamp has a unique cluster-IP 10.104.156.85 , an internal port 30527 and an external port 8080 . Try access the end point with curl 10.104.156.85:8080 . Using labels The Deployment created automatically a label for our Pod. With describe deployment command you can see the name of the label: kubectl describe deployment . This label can be used to query list of Pods: $ kubectl get pods -l run = kubernetes-bootcamp NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5c69669756-rmxrn 1 /1 Running 0 1h The same can be used on services: $ kubectl get services -l run = kubernetes-bootcamp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes-bootcamp NodePort 10 .104.156.85 <none> 8080 :30527/TCP 24m Get name of the pod: export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') Apply a new label to this pod: kubectl label pod $POD_NAME app=v1 And check the pod description: $ kubectl describe pods $ POD_NAME ` Name : kubernetes - bootcamp - 5 c69669756 - rmxrn Namespace : default Node : docker - for - desktop / 192.168 . 65.3 Start Time : Thu , 21 Feb 2019 14 : 51 : 05 - 0700 Labels : app = v1 pod - template - hash = 1725225312 run = kubernetes - bootcamp ... This label can be used to query pods: $ kubectl get pods -l app = v1 NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5c69669756-rmxrn 1 /1 Running 0 1h Delete a service To delete a service, use this command kubectl delete service -l run=kubernetes-bootcamp Confirm the service is gone: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 2h But the pod should still be running: $ kubectl exec -ti $POD_NAME curl localhost:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-5c69669756-rmxrn | v = 1 Scaling a service Scaling is achieved through number of replica in a deployment. Kubernetes also support auto scaling but it's not covered in this part of the tutorial. Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods. Once you have multiple instances of an Application running, you would be able to do Rolling updates without downtime. To scale an existing service, use the following commands. List running deployments: $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1 /1 1 1 37s The DESIRED state is showing the configured number of replicas. The CURRENT state show how many replicas are running now. The UP-TO-DATE is the number of replicas that were updated to match the desired (configured) state. To scale the deployments to replica of 4: $ kubectl scale deployments/kubernetes-bootcamp --replicas = 4 deployment.apps/kubernetes-bootcamp scaled $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 4 /4 4 4 100s We can see the available instances and ready instances are now four. Check if number of pods changed: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kubernetes-bootcamp-6bf84cb898-4jx92 1 /1 Running 0 8s 172 .18.0.7 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-mm2g5 1 /1 Running 0 8s 172 .18.0.5 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-rsml8 1 /1 Running 0 8s 172 .18.0.3 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-wfvkt 1 /1 Running 0 8s 172 .18.0.6 minikube <none> <none> There are 4 pods with different IP now. The changes should also register with deployments. $ kubectl describe deployments / kubernetes - bootcamp Name : kubernetes - bootcamp Namespace : default CreationTimestamp : Sat , 23 Feb 2019 17 : 38 : 57 + 0000 Labels : run = kubernetes - bootcamp Annotations : deployment . kubernetes . io / revision : 1 Selector : run = kubernetes - bootcamp Replicas : 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType : RollingUpdate MinReadySeconds : 0 RollingUpdateStrategy : 25 % max unavailable , 25 % max surge Pod Template : Labels : run = kubernetes - bootcamp Containers : kubernetes - bootcamp : Image : gcr . io / google - samples / kubernetes - bootcamp : v1 Port : 8080 / TCP Host Port : 0 / TCP Environment : < none > Mounts : < none > Volumes : < none > Conditions : Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets : < none > NewReplicaSet : kubernetes - bootcamp - 6 bf84cb898 ( 4 / 4 replicas created ) Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 73s deployment - controller Scaled up replicaset kubernetes - bootcamp - 6 bf84cb898 to 4 Load balancing With replica enabled, load balancing should also automatically enable for this service. $ kubectl describe services / kubernetes - bootcamp Name : kubernetes - bootcamp Namespace : default Labels : run = kubernetes - bootcamp Annotations : < none > Selector : run = kubernetes - bootcamp Type : NodePort IP : 10.97 . 18.242 Port : < unset > 8080 / TCP TargetPort : 8080 / TCP NodePort : < unset > 32111 / TCP Endpoints : 172.18 . 0.3 : 8080 , 172.18 . 0.5 : 8080 , 172.18 . 0.6 : 8080 + 1 more ... Session Affinity : None External Traffic Policy : Cluster Events : < none > There are 4 endpoints on this services, each one is one our the replica. To verify the load balancing is working. Let's make request to this service and each time we should be hitting different pods. $ export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' ) $ echo NODE_PORT = $NODE_PORT NODE_PORT = 32111 $ curl $( minikube ip ) : $NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-6bf84cb898-rsml8 | v = 1 $ curl $( minikube ip ) : $NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-6bf84cb898-4jx92 | v = 1 $ curl $( minikube ip ) : $NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-6bf84cb898-wfvkt | v = 1 As we can see in the curl printout. Each time we hit a different pod. Scale down To scale down the service, issue the same scale command but with a smaller replica number. $ kubectl scale deployments/kubernetes-bootcamp --replicas = 2 deployment.extensions/kubernetes-bootcamp scaled $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 2 /2 2 2 6m26s $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kubernetes-bootcamp-6bf84cb898-4jx92 1 /1 Terminating 0 6m18s 172 .18.0.7 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-mm2g5 1 /1 Running 0 6m18s 172 .18.0.5 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-rsml8 1 /1 Running 0 6m18s 172 .18.0.3 minikube <none> <none> kubernetes-bootcamp-6bf84cb898-wfvkt 1 /1 Terminating 0 6m18s 172 .18.0.6 minikube <none> <none> This confirms that 2 pods are terminating. Rolling updates Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. The new Pods will be scheduled on Nodes with available resources. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods). In Kubernetes, updates are versioned and any Deployments update can be rolled back to previous (stable) version. Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application. Rolling updates allow the following actions: Promote an application from one environment to another (via container image updates) Rollback to previous versions Continuous Integration and Continuous Delivery of applications with zero downtime To roll out an update, follow these steps: List running deployments: $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 4 /4 4 4 24s List running pods: $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-6bf84cb898-dg8zx 1 /1 Running 0 25s kubernetes-bootcamp-6bf84cb898-fzb22 1 /1 Running 0 25s kubernetes-bootcamp-6bf84cb898-mfq7l 1 /1 Running 0 25s kubernetes-bootcamp-6bf84cb898-r8snq 1 /1 Running 0 25s To update the image of the application to version 2, use the set image command, followed by the deployment name and the new image version: $ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp = jocatalin/kubernetes-bootcamp:v2 deployment.apps/kubernetes-bootcamp image updated The command notified the Deployment to use a different image for your app and initiated a rolling update. Check the status of the new Pods, and view the old one terminating with the get pods command: $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5bf4d5689b-4xkmn 1 /1 Running 0 10s kubernetes-bootcamp-5bf4d5689b-hv6qr 1 /1 Running 0 10s kubernetes-bootcamp-5bf4d5689b-jm57j 1 /1 Running 0 13s kubernetes-bootcamp-5bf4d5689b-qvgkz 1 /1 Running 0 13s kubernetes-bootcamp-6bf84cb898-dg8zx 1 /1 Terminating 0 83s kubernetes-bootcamp-6bf84cb898-fzb22 1 /1 Terminating 0 83s kubernetes-bootcamp-6bf84cb898-mfq7l 1 /1 Terminating 0 83s kubernetes-bootcamp-6bf84cb898-r8snq 1 /1 Terminating 0 83s $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5bf4d5689b-4xkmn 1 /1 Running 0 43s kubernetes-bootcamp-5bf4d5689b-hv6qr 1 /1 Running 0 43s kubernetes-bootcamp-5bf4d5689b-jm57j 1 /1 Running 0 46s kubernetes-bootcamp-5bf4d5689b-qvgkz 1 /1 Running 0 46s Old instances are replaced with updated instances eventually. The update can be confirmed also by running a rollout status command: $ kubectl rollout status deployments/kubernetes-bootcamp deployment \"kubernetes-bootcamp\" successfully rolled out To view the current image version of the app, run a describe command against the Pods: kubectl describe pods To roll back an update, let's deploy an update with problems: kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10 And check status: $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 3 /4 2 3 5m29s $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-597cfc5b76-cxccd 0 /1 ImagePullBackOff 0 69s kubernetes-bootcamp-597cfc5b76-h6r48 0 /1 ErrImagePull 0 69s kubernetes-bootcamp-5bf4d5689b-4xkmn 1 /1 Running 0 4m9s kubernetes-bootcamp-5bf4d5689b-jm57j 1 /1 Running 0 4m12s kubernetes-bootcamp-5bf4d5689b-qvgkz 1 /1 Running 0 4m12s Something is wrong with the updated images. To get more insights, use the describe command: kubectl describe pods To roll back the update, issue this command: $ kubectl rollout undo deployments/kubernetes-bootcamp deployment.apps/kubernetes-bootcamp rolled back And we are back to old state: $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-5bf4d5689b-4xkmn 1 /1 Running 0 5m41s kubernetes-bootcamp-5bf4d5689b-j6kp4 1 /1 Running 0 21s kubernetes-bootcamp-5bf4d5689b-jm57j 1 /1 Running 0 5m44s kubernetes-bootcamp-5bf4d5689b-qvgkz 1 /1 Running 0 5m44s Kubernetes Commands check current version: kubectl version get cluster info: kubectl cluster-info list nodes: kubectl get nodes list pods: kubectl get pods describe pods: kubectl describe pods run a deployment: kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080 list deployments: kubectl get deployments","tags":"Kubernetes","url":"https://hazelement.github.io/kubernetes-basics.html"},{"title":"Non-transparent proxy with Squid and Docker","text":"We are looking to create a non-transparent proxy server using Squid for networking sniffing. The end product should be able to sniff both HTTP and HTTPS traffic. Transparent proxy server vs non-transparent proxy server Both types proxy servers are able to relay traffic from client machine. Transparent proxy is the easier setup. It allows quick access to the web for everyone without configuration from client side. The down side of a transparent proxy is that it provides limited function to perform network traffic monitoring and filtering. Only HTTP traffic can be monitored. A non-transparent proxy on the other hand, provides a much more powerful and flexible proxying service. It can relay HTTPS traffic as a man-in-the-middle proxy by forging its own SSL certificate. However, in order to achieve this, server's CA certificate must be installed and trusted as a root certificate on client's machine. In this project, we will explorer setting up a non-transparent proxy server using squid and containerize it into a docker. This container can later be used with an ICAP service to perform network traffic filtering in the future. Squid docker A popular proxy software, link . The Dockerfile is created and the content is as followed. FROM ubuntu:18.04 ENV SQUID_VERSION=3.5.27 \\ SQUID_DIR=/usr/local/squid \\ SQUID_USER=proxy RUN apt-get update RUN apt-get install build-essential openssl libssl1.0-dev wget -y WORKDIR /tmp RUN wget http://www.squid-cache.org/Versions/v3/3.5/squid- ${ SQUID_VERSION } .tar.gz RUN tar -xzf squid- ${ SQUID_VERSION } .tar.gz WORKDIR /tmp/squid- ${ SQUID_VERSION } RUN chmod +x configure RUN ./configure \\ --datadir=/usr/share/squid \\ --sysconfdir=/etc/squid \\ --libexecdir=/usr/lib/squid \\ --with-pidfile=/var/run/squid.pid \\ --with-filedescriptors=65536 \\ --with-large-files \\ --enable-delay-pools \\ --enable-cache-digests \\ --enable-icap-client \\ --enable-ssl \\ --enable-ssl-crtd \\ --with-openssl \\ --enable-follow-x-forwarded-for \\ --with-default-user= ${ SQUID_USER } RUN make RUN make install ENV PATH= $PATH :/usr/local/squid/sbin RUN /usr/lib/squid/ssl_crtd -c -s /var/lib/ssl_db # SSL certificate database directory RUN mkdir -p ${ SQUID_DIR } RUN chmod -R 755 ${ SQUID_DIR } RUN chown -R ${ SQUID_USER } : ${ SQUID_USER } ${ SQUID_DIR } RUN chown -R ${ SQUID_USER } : ${ SQUID_USER } /usr/lib/squid/ssl_crtd EXPOSE 3128/tcp In addition to the Dockerfile , there are 5 more files that are important. /usr/local/squid/var/cache # cache /usr/local/squid/var/logs # logs /etc/squid/squid.conf # squid configuration file /usr/local/squid/ca.crt # ssl certificate /usr/local/squid/ca.key # ssl private key These files will be mapped to the host system in docker-compose.yml file. ca.crt and ca.key are our server SSL certificate and keys. They can be generated using this script. 1 2 3 4 5 #!/bin/sh openssl genrsa -out ca.key 2048 openssl req -new -x509 -days 3650 -key ca.key -out ca.crt -subj \"/CN=XXXXX CA\" openssl genrsa -out cert.key 2048 Replace XXXXX with the name at your choice. squid.conf is squid configuration file. Here is an example #defaults acl localnet src 10.0.0.0/8 acl localnet src 172.16.0.0/12 acl localnet src 192.168.0.0/16 acl localnet src fc00::/7 acl localnet src fe80::/10 acl SSL_ports port 443 acl Safe_ports port 80 acl Safe_ports port 21 acl Safe_ports port 443 acl Safe_ports port 70 acl Safe_ports port 210 acl Safe_ports port 1025-65535 acl Safe_ports port 280 acl Safe_ports port 488 acl Safe_ports port 591 acl Safe_ports port 777 acl CONNECT method CONNECT http_access allow manager localhost http_access deny manager http_access deny !Safe_ports http_access deny CONNECT !SSL_ports http_access allow localnet http_access allow localhost # allow traffic from localhost # to allow other client IP, set them up above http_access deny all # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS # cache_effective_user proxy cache_effective_group proxy # this work for both http and https traffic http_port 3128 ssl-bump generate-host-certificates=on \\ dynamic_cert_mem_cache_size=4MB \\ cert=/usr/local/squid/ca.crt \\ key=/usr/local/squid/ca.key #always_direct allow all ssl_bump server-first all #sslproxy_cert_error deny all sslproxy_flags DONT_VERIFY_PEER sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1 # icap filtering service icap_enable on icap_preview_enable on # request filtering, set bypass to 0 to enforce icap_service service_req reqmod_precache bypass=1 icap://icap:1344 adaptation_access service_req allow all # response filtering, set bypass to 0 to enforce icap_service service_resp respmod_precache bypass=1 icap://icap:1344 adaptation_access service_resp allow all # # Add any of your own refresh_pattern entries above these. # refresh_pattern &#94;ftp: 1440 20% 10080 refresh_pattern &#94;gopher: 1440 0% 1440 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 shutdown_lifetime 1 second At last, the docker-compose.yml that puts everything together. version: '3' services: squid: build: ./ ports: - \"8888:3128\" volumes: - ./cache:/usr/local/squid/var/cache - ./logs:/usr/local/squid/var/logs - ./squid.conf:/etc/squid/squid.conf - ./ca.crt:/usr/local/squid/ca.crt - ./ca.key:/usr/local/squid/ca.key restart: always command: squid -f /etc/squid/squid.conf -NYCd 1 The folder structure should look like this: squid/ ca.crt ca.key cert.key docker-compose.yml Dockerfile squid.conf cache/ logs/ Start the squid docker by running docker-compose up . Docker should start building the docker image. At the very end, the console should have a print out like this. Creating squid_squid_1 ... done Attaching to squid_squid_1 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Current Directory is / tmp / squid-3 . 5 . 27 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Starting Squid Cache version 3 . 5 . 27 for x86_64-pc-linux-gnu ... squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Service Name : squid squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Process ID 1 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Process Roles : master worker squid_1 | 2018 / 09 / 01 02 : 33 : 39 | With 1048576 file descriptors available squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Initializing IP Cache ... squid_1 | 2018 / 09 / 01 02 : 33 : 39 | DNS Socket created at [ :: ] , FD 9 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | DNS Socket created at 0 . 0 . 0 . 0 , FD 10 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Adding nameserver 127 . 0 . 0 . 11 from / etc / resolv . conf squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Adding ndots 1 from / etc / resolv . conf squid_1 | 2018 / 09 / 01 02 : 33 : 39 | helperOpenServers : Starting 1 / 8 'ssl_crtd' processes squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Logfile : opening log daemon :/ usr / local / squid / var / logs / access . log squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Logfile Daemon : opening log / usr / local / squid / var / logs / access . log squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Local cache digest enabled ; rebuild / rewrite every 3600 / 3600 sec squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Store logging disabled squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Swap maxSize 0 + 262144 KB , estimated 20164 objects squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Target number of buckets : 1008 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Using 8192 Store buckets squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Max Mem size : 262144 KB squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Max Swap size : 0 KB squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Using Least Load store dir selection squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Current Directory is / tmp / squid-3 . 5 . 27 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Finished loading MIME types and icons . squid_1 | 2018 / 09 / 01 02 : 33 : 39 | HTCP Disabled . squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Squid plugin modules loaded : 0 squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Adaptation support is on squid_1 | 2018 / 09 / 01 02 : 33 : 39 | Accepting SSL bumped HTTP Socket connections at local = [ :: ] : 3128 remote = [ :: ] FD 16 flags = 9 squid_1 | 2018 / 09 / 01 02 : 33 : 40 | storeLateRelease : released 0 objects Client setup On client machine, install the server certificate ca.crt as Trusted Root Certification Authorities, refer to Windows and Linux , and Mac . Configure the proxy setting on your machine to ip localhost and port 8888 . If the client is on a different machine, you need to open port 8888 on the server side and configure client to the server's ip and port 8888 . Once everything configured, go to google.ca and check its SSL certificate, the certificate should say its issued by XXXXX instead of Google. But all certificate encryption and signature should be valid as we have trusted our own CA certificate on the client machine.","tags":"Networking","url":"https://hazelement.github.io/non-transparent-proxy-with-squid-and-docker.html"},{"title":"Compile tcpdump for android.","text":"There are many ways to sniff network traffic on android, VPN, proxy and etc. Today we are gonna look into using compiling tcpdump for Android which can be used with netcat to sniff network traffic later. The Linux binaries We will be using two Linux binaries to achive this, tcpdump and netcat. Tcpdump is a popular tool in Linux to capture net traffic. Netcat is another Linux binary that are commonly used to listen on a socket. We will be using these two binaries along with Java coded android apps to demonstrate the technique. A Rooted Android Phone First thing first, this technique requires a rooted android phone. If your phone is not rooted, check out some posts and root your device first. Compile Tcpdump and Netcat Binaries for Android Like any other binaries written in C, we need to compile them differerntly if we want to run them on differernt platforms. Let's start by installing our android compiler, assuming we are compiling for arm processor architecture. Execute the following command in your Ubuntu shell. sudo apt-get install gcc-arm-linux-gnueabi sudo apt-get install byacc sudo apt-get install flex This will install gcc for arm architectre and other support tools for compiling. Next, create a folder named \"compile_for_android\", this is where we will be performing all the compiling. mkdir compile_for_android cd compile_for_android Now let's download tcpdump source code. wget http://www.tcpdump.org/release/tcpdump-4.8.1.tar.gz Tcpdump depends on libpcap, so we need to download and compile libpcap source code as well. wget http://www.tcpdump.org/release/libpcap-1.8.1.tar.gz Extract these two packages. tar zxvf tcpdump-4.8.1.tar.gz tar zxvf libpcap-1.8.1.tar.gz Now we are ready to compile our tcpdump. First, let's make sure our compiler is the android compiler. export CC = arm-linux-gnueabi-gcc Compiler libpcap first. cd libpcap-1.8.1 ./configure --host = arm-linux --with-pcap = linux make This should compiler the libpcap library for us. Now let's go to our tcpdump directory. cd .. cd tcpdump-4.8.1 Before we perform the same thing above, there is a few things we need to do. Figure out what major version our Ubuntu we have, uname -a This will give out something like this. 4 .2.0-42-generic In this case, our major version is 4 and we set a variable in command. export ac_cs_linux_vers = 4 Set the following variables to make our binary self contained (ie. not reliant on other libraries). export CFLAGS = -static export CPPFLAGS = -static export LDFLAGS = -static And configure the directory, ./configure --host = arm-linux --disable-ipv6 And then make it, make Strip the symbol information to make binary smaller. These symbols are only useful in debugging the application. arm-linux-gnueabi-strip tcpdump","tags":"Android","url":"https://hazelement.github.io/compile-tcpdump-for-android.html"},{"title":"Setup Jenkins with Django for Continous Deployment","text":"About Jenkins Jenkins is a tool that is widely used for continous integration/deployment. It's basically a tool to automate the process of writing code, running tests and deploy for production. We will be using this post to demonstrate the setup using a \"hello world\" Django web application. The web application will be served using apache. However, the setup can be used with any web development language and framework. The Setup Our development code will be sitting at ~/django_hello_world. For simplicity, the source code will be pushed to a git repository that is sitting on the same machine at /webapp_repo. It can be on other machine through a ssh tunnel. The production code, which is also our live code, will be sitting at /webapps/django_hello_world Django Hello World First thing first, we need to setup an web application first. This will be a similar version of the official Django tutorial, https://docs.djangoproject.com/en/1.10/intro/tutorial01/ . Start the Django project: cd ~/django_hello_world django-admin startproject hello_world Under the same directory, create an app called polls: python manage.py startapp polls This will create a folder called polls. Under polls, open up the file called views and put the following code: from django.http import HttpResponse def index ( request ): return HttpResponse ( \"Hello, world. You're at the polls index.\" ) In the same polls folder, create a file named \"urls.py\" which will define the urls. In the urls file, enter the following code: from django.conf.urls import url from . import views urlpatterns = [ url ( r '&#94;$' , views . index , name = 'index' ), ] Now let's link this url file to the global url file. Open up the urls.py file under hello world folder and put in the following code: from django.conf.urls import url , include from django.contrib import admin urlpatterns = [ url ( r '&#94;polls/' , include ( 'polls.urls' )), url ( r '&#94;admin/' , admin . site . urls ), ] And that's it! Go back to project root directory: cd ~/django_hello_world And run this command: python manage.py runserver Open up http://localhost:8000/polls/ in the browser and we should see the text \"Hello, world. You're at the polls index.\" Remember this only runs our web application under local host, and only we can see it. In order for other people to see it, we need to use apache to serve it. Serve Django with Apache Django's tutorial website has a thorough documentation on the setup, https://docs.djangoproject.com/en/1.10/howto/deployment/wsgi/modwsgi/ . We will briefly mention it here. Using mod_wsgi daemon mode is the recommended way to serve our application. Assuming we have apache and mod_wsgi installed. First of all, let's create a directory to store our production files where Jenkins will be publishing to. cd / mkdir /webapps/hello_world Our production code will be sitting under this directory, hello_world/ hello_world/ polls/ ... Under apache's enabled site directory, let's create a conf file for our web application. cd /etc/apache2/sites-enabled touch hello_world.conf In hello_world.conf, enter the following contents, Listen 8888 <VirtualHost *:8888> # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request's Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com # ServerAdmin webmaster@localhost # DocumentRoot /var/www/html # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. # LogLevel info ssl:warn WSGIScriptAlias / /webapps/hello_world/hello_world/wsgi.py WSGIDaemonProcess helloworld.com python-path = /django_hello_world WSGIProcessGroup helloworld.com <Directory> /webapps/hello_world/hello_world> <Files wsgi.py> Require all granted </Files> </Directory> ErrorLog ${ APACHE_LOG_DIR } /error.log CustomLog ${ APACHE_LOG_DIR } /access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with \"a2disconf\". #Include conf-available/serve-cgi-bin.conf </VirtualHost> This document tells apache about configurations of our site. Apache will be listening on port 8888 and our site should be accessible on port 8888 at all IP addresses. There is a wsgi.py file we need to create for apache to load the Django application. Go back to our development directory, cd ~/django_hello_world/hello_world cd hello_world touch wsgi.py The file structure should look like this, hello_world/ hello_world/ wsgi.py ... polls/ ... In wsgi.py, enter the following content, import os from django.core.wsgi import get_wsgi_application os . environ . setdefault ( \"DJANGO_SETTINGS_MODULE\" , \"hello_world.settings\" ) application = get_wsgi_application () And that's it, we are set for apache. Now let's get our code into our production directory. Setup Git Repository Next setup is to setup the git repository and pull code to our production directory. For simplicity, we will setup the git repository as a local directory on our machine. cd / mkdir git_repo cd git_repo git init --bare hello_world.git This will setup a local git repository under /git_repo called hello_world.git. Next, let setup our development code to track this directory and push our code to it. cd /django_hello_world/hello_world git remote add origin /git_repo/hello_world.git Make our first commit by typing, git status git add --all git commit -a Type in our first commit message and finish the commit. Next let's push our first commit to git repository, git push -u origin master This will push our commit to remote called origin and setup our local master to track the remote master branch. Next let's pull our code into our production directory. cd /webapps git clone /git_repo/hello_world.git Our lastest code should show up in the webapps directory. This is also where apache will be accessing our site code. Restart apache server, sudo systemctl restart apache2 Now, we should be able to see the site under 8888 port, try localhost:8888 or 127.0.0.1:8888 in our web browser. Setup Jenkins to Link Everything First thing first, let's install Jenkins. Follow the instructions on the website, https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins+on+Ubuntu . wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' sudo apt-get update sudo apt-get install jenkins Jenkins need JDK and JRE installed to run, install them first if your machine doesn't have them. After installation, check status of Jenkins and make sure it's running, sudo service jenkins status If Jenkins is not running, start it with, sudo service jenkins start Now, open up browser and go to localhost:8080, this is where Jenkins is served. At first time, Jenkins will ask your to create an admin account, just follow the steps to create the admin account. Create a New Build Configuration At Jenkins's home page, click New Item to create an entry for our project. Enter \"hellow world\" for the item name and select Freestyle project . On the next pages, where are a few tabs we need to go through. Under Source Code Management , select Git . A new sub window will appear. Enter \"/git_repo/hello_world.git\" for Repository URL* . This is where we setup our git repository. Leave Credentials as \"none\" as we don't have authentification to access this repository. Leave Brances to build as \"*/master\". Under Build Triggers , check Build periodically , and enter \"H/10 * * * *\" fpr Schedule , this will check for any changes every 10 minutes. Also check Build when a change is pushed to GitHub , this will trigger Jenkins to run whenever a change checked in. Under Build , this is where we will be entering our build script, type in the following, cd /webapps/hello_world python manage.py migrate python manage.py test --noinput hello_world polls sudo systemctl restart apache2 This is also where we can run our test scripts before make our changes live. Click save and we should be good. In the next page, which is also where our project dash board, click Build Now and once its finished, we can see the latest build result under Build History . Make a Simple Change and See It Becomes Live Let's make a change to our source code and test if our Jenkins does the job. cd ~/django_hello_world/hello_world/polls In the \"views.py\" file, we had the code as following, from django.http import HttpResponse def index ( request ): return HttpResponse ( \"Hello, world. You're at the polls index.\" ) from django.http import HttpResponse def index ( request ): return HttpResponse ( \"Hello, world. You're at the polls index. An update on the polls index from Jenkins\" ) Commit and push the new changes, git commit -a git push Now go to localhost:8888, we should be able to see our change becomes live once Jenkins finish the new build. And that's it, we just setup our first continous integration system.","tags":"Jenkins","url":"https://hazelement.github.io/setup-jenkins-with-django-for-continous-deployment.html"},{"title":"Setup Pelican for Github User Pages","text":"For those who don't know. Github has this Pages utility which allow each user to setup a personal sub-domain under Github, functioning like a personal blog. It uses a unique git repo for each user to server its file content. For details, check out their website, https://pages.github.com . There are two types of Github Pages, Project Pages and User Pages. Setting up Project Pages is straight forward. I will be discussing setting up User Pages and some tricks I used to streamline the publishing process. About Github Pages Files of a Github page reside in a very specific Github repository that is owned and unique to each user. They look like this format, yourname.github.io, where \"yourname\" is your Github name. The repository has to follow this format otherwise, it won't work. Go ahead and create a new repository named yourname.github.io. Once that's setup, you can see it on your Github profile and it is waiting for a init push. Now let's get back to our Pelican project. We don't need to clone it to our local disk. Pushings Pelican Output to Github Repo The idea is basically to push Pelican's output folder to the Github repository we just created. To achive this, we have a great tool called ghp-import . We can install it easily with the following command: pip install ghp-import A normal command using ghp-import is like follows: pelican content -o output -s pelicanconf.py ghp-import output git push git@github.com:username/username.github.io.git gh-pages:master The first line generate all rst documents into our output folder. ghp-import then import this output folder into a git branch called gh-pages. Then the last line push this branch to its remote repository which is our Github User Pages repo, yourname.github.io. gh-pages is our local branch and master is our remote repository branch. Please note that the remote branch must be the master branch for it to work properly. The last command will prompt your enter your username and password. Simply enter that, our output folder should be pushed to the repo successfully. Now open up browser and enter username.github.io and we can see our blog live on the web. Github 2-step Authentification If you have 2-step Authentification setup on Github account, using the git push command might not work for you as it doesn't implement a way for you to enter the second passcode. There are 2 ways to solve this problem, one way is to generate a app hash string passcode from Github and use it while doing git push. It's annoying as we have to keep that somewhere and we need to enter that everytime we perform a push. Another easier way is to generate a ssh and put your public key to github. Github SSH Access There are many tutorials on how to generate a SSH key pair. A SSH key pair contains a private key and a public key. A private key is our personal key and we should alway keep secret. A public key is like a lock that matches our private key. When we need to access some machine remotely, we give public key to remote mahine manager and he can install it to his machine. It's like installing our lock onto his house's front door (in this case, his house would be the machine). So we can get into his house with our prviate key. And of course, he can setup locks that comes from other people, ie his door can have many locks and each lock can open his door. If you are using a linux machine, a quick way to generate a key pair is to use ssh-keygen. ssh-keygen -t rsa -C \"your_email@example.com\" This will generate two files, id_rsa, and id_rsa.pub. Open id_rsa.pub with any text editor and copy its entire content. Now let's put this public key to our Github account so that we can use our private key to access Github. In https://github.com/settings/keys , we have a section to add new public key. Paste everything from public key into \"Key\" section and name \"title\" to \"mySSHKey\". And click on add SSH key. We should be good to go. Next time when we do a git push, it should stop asking us about our password. Streamline With A Script We can stream line the publishing process with a bash script. Let's first go to our project root folder and create a file called pubish. touch publish In this file, let's enter the following content #!/bin/bash pelican content -o output -s pelicanconf.py ghp-import output git push git@github.com:username/username.github.io.git gh-pages:master Making this file executable: chmod +x publish We just created a script to perform the publishing process for us. To publish new content, simply enter this command under project root directory. ./publish It should perform all the task for us.","tags":"Pelican","url":"https://hazelement.github.io/setup-pelican-for-github-user-pages.html"},{"title":"Using Pelican for Blogging","text":"Pelican is a popular static website generator written in Python. It saves bloggers from worrying about formats so that they can focus on the content itself. Pelican to bloggers is like Latex to document writters. Pelican take advantage of Markdown and reStructured text (rst) to generated formatted texts. I wrote this website usig rst. Markdown is also a good option as well. Get started Pelican has a great tutorial covering the basic steps to setup a website to play with. http://docs.getpelican.com/en/stable/quickstart.html . Here is a brief summary of that page. Installation To install pelican with pip: pip install pelican If we are using markdown, we can install it with pip too: pip install markdown Create a project A directory must be created for our new project: mkdir -p ~/pelican_tutorial cd pelican_tutorial Once we are in our project directory, we can create a project using the following command: pelican-quickstart Pelican then will ask us a few questions regarding your website. Don't worry if we are not sure on some of these questions, all these options can be changed afterwards. An example of these questions are here: Welcome to pelican-quickstart v3.6.3. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? my_first_blog > Who will be the author of this web site? haze > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) n > Do you want to enable article pagination? ( Y/n ) y > How many articles per page do you want? [ 10 ] > What is your time zone? [ Europe/Paris ] > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) y > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) y > Do you want to upload your website using FTP? ( y/N ) n > Do you want to upload your website using SSH? ( y/N ) n > Do you want to upload your website using Dropbox? ( y/N ) n > Do you want to upload your website using S3? ( y/N ) n > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) n > Do you want to upload your website using GitHub Pages? ( y/N ) n Done. Your new project is available at /xxx/pelican_tutorial Create an articles with category Next we are going to create our first post with a category specified. In plelican, each post is a rst file stored within the cotent directory. ~/pelican_tutorial/content Although, categories can be specified within rst file similar to a tag. I prefer to take advantage of folders to put my rst files into each category. In the content folder, if we created folders and put our rst file in each sub folder. Then each folder will be considered as a category. ~/pelican_tutorial/tutorial We just created a tutorial category. And let's create our first post under this category. cd ~/pelican_tutorial/tutorial touch myfirst_tutorial.rst We can then input content to this rst file. For example: My first tutorial ######################## :date: 2016-11-19 11:30 :tags: reStructured text, rst :authors: Haze ===== Title ===== Subtitle -------- This is a paragraph. Save this file, and we are ready to generate our first post into html file. Generate site From site root directory cd ~/pelican_tutorial Run the following code to generate your site: pelican content A folder called output will be generated. This is where our site sits. To see how our site looks like, enter output directory: cd output Run the local pelican server: python -m pelican.server Open up web browser, and type in http://localhost:8000/ , we should see the website served from local directory. Some tips Autosite updates Usually we would like to see our website updates live while we changing the rst file contents, especially during development. This can be achieved by running the following command. make regenerate \"make\" is a script at the project root folder. Don't close terminal after running this command as the script is monitoring our project folder to detect any changes. We can continous editing and saving your rst file. All changes will be reflected on your local website. Althought we need to refresh the page of course. One drawback with this script is that if we have a syntax error in the rst file, it will likely crash the script and we would have to restart it again after fixing the syntax. For popular rst syntax, check out my other post, reStructured Text Syntax . Next up, Setup Pelican for Github User Pages .","tags":"Pelican","url":"https://hazelement.github.io/using-pelican-for-blogging.html"},{"title":"reStructured Text Syntax","text":"A page with popular reStructured Text Syntax A page with popular reStructured Text Syntax A page with popular reStructured Text Syntax Title Subtitle This is a paragraph. Paragraphs line up at their left edges, and are normally separated by blank lines. Plain text Typical result This is a normal text paragraph. The next paragraph is a code sample: It is not processed in any way, except that the indentation is removed. It can span multiple lines. This is a normal text paragraph again. Bullet lists: This is item 1 This is item 2 Bullets are \"-\", \"*\" or \"+\". Continuing text must be aligned after the bullet and whitespace. Note that a blank line is required before the first item and after the last, but is optional between items. Enumerated lists: This is the first item This is the second item Enumerators are arabic numbers, single letters, or roman numerals List items should be sequentially numbered, but need not start at 1 (although not all formatters will honour the first index). This item is auto-enumerated Definition lists: what Definition lists associate a term with a definition. how The term is a one-line phrase, and the definition is one or more paragraphs or body elements, indented relative to the term. Blank lines are not allowed between term and definition. Authors: Tony J. (Tibs) Ibbs, David Goodger (and sundry other good-natured folks) Version: 1.0 of 2001/08/08 Dedication: To my father. A paragraph containing only two colons indicates that the following indented or quoted text is a literal block. Whitespace, newlines, blank lines, and all kinds of markup (like *this* or \\this) is preserved by literal blocks. The paragraph containing only '::' will be omitted from the result. The :: may be tacked onto the very end of any paragraph. The :: will be omitted if it is preceded by whitespace. The :: will be converted to a single colon if preceded by text, like this: It's very convenient to use this form. Literal blocks end when text returns to the preceding paragraph's indentation. This means that something like this is possible: We start here and continue here and end here. Per-line quoting can also be used on unindented literal blocks: > Useful for quotes from email and > for Haskell literate programming. Grid table: Header 1 Header 2 Header 3 body row 1 column 2 column 3 body row 2 Cells may span columns. body row 3 Cells may span rows. Cells contain blocks. body row 4 Simple table: Inputs Output A B A or B False False False True False True False True True True True True External hyperlinks, like Python . Internal crossreferences, like example .","tags":"reStructuredText","url":"https://hazelement.github.io/restructured-text-syntax.html"}]}